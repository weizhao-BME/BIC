{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.io import loadmat, savemat\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import os\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "def bootstrap_reg(model,x_train,y_train,x_test,y_test, times = 100):\n",
    "    '''\n",
    "    Perform bootstrapping, store results for further analysis and visualization\n",
    "    :param x_train: training set X\n",
    "    :param y_train: training set Y\n",
    "    :param x_test: testing set X\n",
    "    :param y_test: testing set Y\n",
    "    :param featrue_eng: feature engineering method list to pass in feature engineering function\n",
    "    :param times: how many times to bootstrap\n",
    "    :return: dictionary of metrics, dict['<metric name>'] = [<values, length = fold>]\n",
    "    '''\n",
    "    mae_results = []\n",
    "    rmse_results =[]\n",
    "    r2_results = []\n",
    "    pearson_results = []\n",
    "    spearman_results =[]\n",
    "    index = np.arange(x_train.shape[0])\n",
    "    for i in range(times):\n",
    "        boot_index = resample(index, replace=True, n_samples=None, random_state=9001+i)\n",
    "        x_boot, y_boot = x_train[boot_index], y_train[boot_index]\n",
    "        model.fit(x_boot,y_boot)\n",
    "        y_true = y_test.reshape(-1,)\n",
    "        y_preds = model.predict(x_test).reshape(-1,)\n",
    "        \n",
    "        mae = mean_absolute_error(y_true, y_preds)\n",
    "        rmse = np.sqrt(mean_squared_error(y_true, y_preds))\n",
    "        r2 = r2_score(y_true, y_preds)\n",
    "        p_cor, p_p = pearsonr(y_true,y_preds)\n",
    "        sp_cor, sp_p = spearmanr(y_true, y_preds)\n",
    "        mae_results.append(mae)\n",
    "        rmse_results.append(rmse)\n",
    "        r2_results.append(r2)\n",
    "        pearson_results.append(p_cor)\n",
    "        spearman_results.append(sp_cor)\n",
    "    \n",
    "    #MAE\n",
    "    mae_arr = np.array(mae_results)\n",
    "    mean_mae = np.mean(mae_arr, axis=0)\n",
    "    mae_arr_sorted = np.sort(mae_arr, axis=0)\n",
    "    ci_low = mae_arr_sorted[round(0.025 * times)]\n",
    "    ci_high = mae_arr_sorted[round(0.975 * times)]\n",
    "    mae_result = {'result': mae_arr, 'mean': mean_mae, 'CI': [ci_low, ci_high]}\n",
    "    \n",
    "\n",
    "    # RMSE\n",
    "    rmse_arr = np.array(rmse_results)\n",
    "    mean_rmse = np.mean(rmse_arr, axis=0)\n",
    "    rmse_arr_sorted = np.sort(rmse_arr, axis=0)\n",
    "    ci_low = rmse_arr_sorted[round(0.025 * times)]\n",
    "    ci_high = rmse_arr_sorted[round(0.975 * times)]\n",
    "    rmse_result = {'result': rmse_arr, 'mean': mean_rmse, 'CI': [ci_low, ci_high]}\n",
    "\n",
    "    # R2\n",
    "    r2_arr = np.array(r2_results)\n",
    "    mean_r2 = np.mean(r2_arr, axis=0)\n",
    "    r2_arr_sorted = np.sort(r2_arr, axis=0)\n",
    "    ci_low = r2_arr_sorted[round(0.025 * times)]\n",
    "    ci_high = r2_arr_sorted[round(0.975 * times)]\n",
    "    r2_result = {'result': r2_arr, 'mean': mean_r2, 'CI': [ci_low, ci_high]}\n",
    "\n",
    "    # PR\n",
    "    pearson_arr = np.array(pearson_results)\n",
    "    mean_pearson = np.mean(pearson_arr, axis=0)\n",
    "    pearson_arr_sorted = np.sort(pearson_arr, axis=0)\n",
    "    ci_low = pearson_arr_sorted[round(0.025 * times)]\n",
    "    ci_high = pearson_arr_sorted[round(0.975 * times)]\n",
    "    pearson_result = {'result': pearson_arr, 'mean': mean_pearson, 'CI': [ci_low, ci_high]}\n",
    "\n",
    "    # SR\n",
    "    spearman_arr = np.array(spearman_results)\n",
    "    mean_spearman = np.mean(spearman_arr, axis=0)\n",
    "    spearman_arr_sorted = np.sort(spearman_arr, axis=0)\n",
    "    ci_low = spearman_arr_sorted[round(0.025 * times)]\n",
    "    ci_high = spearman_arr_sorted[round(0.975 * times)]\n",
    "    spearman_result = {'result': spearman_arr, 'mean': mean_spearman, 'CI': [ci_low, ci_high]}\n",
    "    \n",
    "    boot_result = {'mae_result': mae_result, 'rmse_result': rmse_result,'r2_result': r2_result, 'pearson_result': pearson_result,'spearman_result': spearman_result}\n",
    "    return boot_result\n",
    "\n",
    "def _returnRow(list):\n",
    "    return ';'.join([str(i) for i in list])\n",
    "\n",
    "# Print results\n",
    "def fillTable(boot_result, digit = 4):\n",
    "    i=0\n",
    "    one_row = [str(round(boot_result['mae_result']['mean'],digit)),\n",
    "               '['+str(round(boot_result['mae_result']['CI'][0],digit))+','+str(round(boot_result['mae_result']['CI'][1],digit))+']',\n",
    "               str(round(boot_result['rmse_result']['mean'],digit)),\n",
    "               '['+str(round(boot_result['rmse_result']['CI'][0],digit))+','+str(round(boot_result['rmse_result']['CI'][1],digit))+']',\n",
    "               str(round(boot_result['r2_result']['mean'],digit)),\n",
    "               '['+str(round(boot_result['r2_result']['CI'][0],digit))+','+str(round(boot_result['r2_result']['CI'][1],digit))+']',\n",
    "               str(round(boot_result['pearson_result']['mean'],digit)),\n",
    "               '['+str(round(boot_result['pearson_result']['CI'][0],digit))+','+str(round(boot_result['pearson_result']['CI'][1],digit))+']',\n",
    "                str(round(boot_result['spearman_result']['mean'],digit)),\n",
    "               '['+str(round(boot_result['spearman_result']['CI'][0],digit))+','+str(round(boot_result['spearman_result']['CI'][1],digit))+']']\n",
    "    print(_returnRow(one_row))\n",
    "\n",
    "def saveresult(boot_result,file):\n",
    "    np.save(file+'mae.npy',boot_result['mae_result']['result'])\n",
    "    np.save(file+'rmse.npy',boot_result['rmse_result']['result'])\n",
    "    np.save(file+'r2.npy',boot_result['r2_result']['result'])\n",
    "    np.save(file+'pearson.npy',boot_result['pearson_result']['result'])\n",
    "    np.save(file+'spearman.npy',boot_result['spearman_result']['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset 1, bootstrapping\n",
    "from sklearn.model_selection import train_test_split\n",
    "#Load Y\n",
    "dataset_MPS95 = np.load('Lab impact_MPS95.npy')\n",
    "#Load X\n",
    "metrics  = ['BAM','BrIC','CP','GAMBIT','HIC','HIP','PRHIC','RIC','SI','PCS','lin_acc_CG_max','ang_vel_max','ang_acc_max','Damage_C','RVCI','KLC','BRIC','CIBIC']\n",
    "for metric in metrics:\n",
    "    HM1_metric = loadmat('Lab impact1_BIC.mat')[metric]\n",
    "    HM2_metric = loadmat('Lab impact2_BIC.mat')[metric]\n",
    "    NFL53_metric = loadmat('Lab impact3_BIC.mat')[metric]\n",
    "    dataset_metric = np.row_stack((HM1_metric,HM2_metric, NFL53_metric))\n",
    "    Y = dataset_MPS95\n",
    "    X = dataset_metric\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X,Y,test_size = 0.2, random_state=9001)\n",
    "    lr = LinearRegression()\n",
    "    boot_result = bootstrap_reg(lr,x_train,y_train,x_test,y_test, times = 100)\n",
    "    saveresult(boot_result,'dataset1_MPS95_'+metric)\n",
    "    fillTable(boot_result,digit = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset 1, bootstrapping\n",
    "from sklearn.model_selection import train_test_split\n",
    "#Load Y\n",
    "dataset_MPSCC95 = np.load('Lab impact_MPSCC95.npy')\n",
    "#Load X\n",
    "metrics  = ['BAM','BrIC','CP','GAMBIT','HIC','HIP','PRHIC','RIC','SI','PCS','lin_acc_CG_max','ang_vel_max','ang_acc_max','Damage_C','RVCI','KLC','BRIC','CIBIC']\n",
    "for metric in metrics:\n",
    "    HM1_metric = loadmat('Lab impact1_BIC.mat')[metric]\n",
    "    HM2_metric = loadmat('Lab impact2_BIC.mat')[metric]\n",
    "    NFL53_metric = loadmat('Lab impact3_BIC.mat')[metric]\n",
    "    dataset_metric = np.row_stack((HM1_metric,HM2_metric, NFL53_metric))\n",
    "    Y = dataset_MPSCC95\n",
    "    X = dataset_metric\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X,Y,test_size = 0.2, random_state=9001)\n",
    "    lr = LinearRegression()\n",
    "    boot_result = bootstrap_reg(lr,x_train,y_train,x_test,y_test, times = 100)\n",
    "    saveresult(boot_result,'dataset1_MPSCC95_'+metric)\n",
    "    fillTable(boot_result,digit = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset 1, bootstrapping\n",
    "from sklearn.model_selection import train_test_split\n",
    "#Load Y\n",
    "dataset_CSDM = np.load('Lab impact_CSDM.npy')\n",
    "#Load X\n",
    "metrics  = ['BAM','BrIC','CP','GAMBIT','HIC','HIP','PRHIC','RIC','SI','PCS','lin_acc_CG_max','ang_vel_max','ang_acc_max','Damage_C','RVCI','KLC','BRIC','CIBIC']\n",
    "for metric in metrics:\n",
    "    HM1_metric = loadmat('Lab impact1_BIC.mat')[metric]\n",
    "    HM2_metric = loadmat('Lab impact2_BIC.mat')[metric]\n",
    "    NFL53_metric = loadmat('Lab impact3_BIC.mat')[metric]\n",
    "    dataset_metric = np.row_stack((HM1_metric,HM2_metric, NFL53_metric))dataset_metric = np.load('dataset1_'+metric+'.npy')\n",
    "    #print(dataset_metric.shape)\n",
    "    Y = dataset_CSDM\n",
    "    X = dataset_metric\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X,Y,test_size = 0.2, random_state=9001)\n",
    "    lr = LinearRegression()\n",
    "    boot_result = bootstrap_reg(lr,x_train,y_train,x_test,y_test, times = 100)\n",
    "    saveresult(boot_result,'dataset1_CSDM_'+metric)\n",
    "    fillTable(boot_result,digit = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###---Similar codes for CF/MMA/NHTSA/NASCAR---###"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
